## Replaybuffer

### 存储的数据：

- obs:[buffer_size+1,n_rollout_threads,num_agnets,obs_shape]
- actions:[buffer_size,n_rollout_threads,num_agnets,act_shape]
- reward,masks,bad_masks,$\pi_a$:[buffer_size,n_rollout_threads,num_agnets,1]
- $V(o)$,$R(o)$:[buffer_size+1,n_rollout_threads,num_agnets,1]
- RNNactor隐状态,RNNCritic隐状态:[buffer_size+1,n_rollout_threads,num_agents,recurrent_hidden_layers,recurrent_hidden_size]

### 参数
- buffer_size:缓冲池大小  200
- n_rollout_threads:采样线程数 1
- num_agents ：智能体数 2
- gamma ： 折扣因子 0.99
- use_proper_time_limits ：用于计算回报的参数？  False
- use_gae ：是否生成通用优势估计 True
- gae_lambda：计算GAE的$\lambda$参数 0.95
- recurrent_hidden_size：RNN隐藏层大小 128
- recurrent_hidden_layers： RNN隐藏层数 1

### 方法

#### 1.insert
$o_{t+1}$,$a_{t}$,$r_{t}$,$mask_{t+1}$,$\pi_a(t)$,$V(o_t)$,$ha_{t+1}$,$hc_{t+1}$

#### 2.compute_returns
计算$R(o)$

#### 3.recurrent_generator
为循环神经网络(RNN)训练准备数据。它处理强化学习中的经验回放缓冲区数据，并将其组织成适合RNN训练的序列批次。而不是从随机打乱的、无关联的时间步中学习。序列长度为data_chunk_length。

生成数据:
[(obs_batch, actions_batch, masks_batch, old_action_log_probs_batch, advantages_batch,returns_batch, value_preds_batch, rnn_states_actor_batch, rnn_states_critic_batch)]

## BCPolicy

BC策略由MLP,RNN,ACT组成。

### 参数

- gain:调整初始化尺度 0.01
- hidden_size:隐藏层 128 128
- act_hidden_size ：输出隐藏层 128 128
- activation_id：激活函数选择，[Tanh(), ReLU(), LeakyReLU(), ELU()]  1(ReLU)
- use_feature_normalization:使用层归一化 False
- use_recurrent_policy：使用循环网络  True
- recurrent_hidden_size:循环网络隐藏层大小 128
- recurrent_hidden_layers：循环网络隐藏层数 1
- lr ：学习率 5e-4
- 优化器 ： Adam

### 方法

#### load_expert_model

加载专家模型参数，可以输入专家模型地址